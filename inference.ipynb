{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb52208",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5642bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import experiment\n",
    "import utils\n",
    "import clip_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcd21b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new experiment folder: experiments/merged_pipline/run_2025-01-31-22-20-23\n"
     ]
    }
   ],
   "source": [
    "output_path = experiment.setup_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43edf63",
   "metadata": {},
   "source": [
    "## Step 1: Instance/Group Feature Extraction for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a8d1d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ exp_dir=experiments/merged_pipline/run_2025-01-29-20-44-08/openscene\n",
      "+ config=./config/openscene/replica/replica_lseg.yaml\n",
      "+ feature_type=distill\n",
      "+ mkdir -p experiments/merged_pipline/run_2025-01-29-20-44-08/openscene\n",
      "+ result_dir=experiments/merged_pipline/run_2025-01-29-20-44-08/openscene\n",
      "+ export PYTHONPATH=models/openscene\n",
      "+ PYTHONPATH=models/openscene\n",
      "+ python -u models/openscene/run/evaluate_merged.py --config=./config/openscene/replica/replica_lseg.yaml feature_type distill save_folder experiments/merged_pipline/run_2025-01-29-20-44-08/openscene\n",
      "++ date +%Y%m%d_%H%M\n",
      "+ tee -a experiments/merged_pipline/run_2025-01-29-20-44-08/openscene/eval-20250129_2044.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__:1.12.1+cu113\n",
      "torch.version.cuda:11.3\n",
      "torch.backends.cudnn.version:8302\n",
      "torch.backends.cudnn.enabled:True\n",
      "[2025-01-29 20:45:00,391 evaluate_merged.py line 167] arch_3d: MinkUNet18A\n",
      "data_root: dataset/data/replica_split\n",
      "data_root_2d_fused_feature: data/replica_multiview_openseg\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:6787\n",
      "distributed: False\n",
      "eval_iou: False\n",
      "exp_dir: ./experiments/openscene/replica_split\n",
      "feature_2d_extractor: lseg\n",
      "feature_type: distill\n",
      "input_color: False\n",
      "labelset: replica\n",
      "manual_seed: 3407\n",
      "mark_no_feature_to_unknown: True\n",
      "model_path: https://cvg-data.inf.ethz.ch/openscene/models/matterport_lseg.pth.tar\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "prompt_eng: True\n",
      "rank: 0\n",
      "save_feature_as_numpy: True\n",
      "save_folder: experiments/merged_pipline/run_2025-01-29-20-44-08/openscene\n",
      "split: all\n",
      "sync_bn: False\n",
      "test_batch_size: 1\n",
      "test_gpu: [0]\n",
      "test_repeats: 1\n",
      "test_workers: 0\n",
      "use_apex: False\n",
      "use_augmentations: False\n",
      "use_shm: False\n",
      "vis_gt: False\n",
      "vis_input: True\n",
      "vis_pred: True\n",
      "voxel_size: 0.02\n",
      "world_size: 1\n",
      "Use prompt engineering: a XX in a scene\n",
      "Loading CLIP ViT-B/32 model...\n",
      "Finish loading\n",
      "[2025-01-29 20:46:08,313 evaluate_merged.py line 291] \n",
      "Evaluation 1 out of 1 runs...\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/test/office4_mesh.ply\u001b[0;m\n",
      " 12%|█▎        | 1/8 [00:40<04:41, 40.19s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/test/room2_mesh.ply\u001b[0;m\n",
      " 25%|██▌       | 2/8 [01:07<03:14, 32.45s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/train/office0_mesh.ply\u001b[0;m\n",
      " 38%|███▊      | 3/8 [01:28<02:17, 27.51s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/train/office1_mesh.ply\u001b[0;m\n",
      " 50%|█████     | 4/8 [01:42<01:28, 22.03s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/train/office2_mesh.ply\u001b[0;m\n",
      " 75%|███████▌  | 6/8 [02:40<00:52, 26.09s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/val/office3_mesh.ply\u001b[0;m\n",
      " 88%|████████▊ | 7/8 [03:17<00:29, 29.65s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: dataset/data/replica_split/val/room1_mesh.ply\u001b[0;m\n",
      "100%|██████████| 8/8 [03:37<00:00, 27.16s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ exp_dir=experiments/merged_pipline/run_2025-01-29-20-44-08/mask3d\n",
      "+ mkdir -p experiments/merged_pipline/run_2025-01-29-20-44-08/mask3d\n",
      "+ result_dir=experiments/merged_pipline/run_2025-01-29-20-44-08/mask3d\n",
      "+ export PYTHONPATH=.\n",
      "+ PYTHONPATH=.\n",
      "+ python -u models/Mask3D/predict.py general.checkpoint=models/Mask3D/checkpoints/scannet/scannet_val.ckpt general.data_dir=dataset/data/replica_split general.save_dir=experiments/merged_pipline/run_2025-01-29-20-44-08/mask3d general.split=all general.required_confidence=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/54/blessman/ml3d/dataset\n",
      "Running on device:  cuda\n",
      "{'_target_': 'models.Res16UNet34C', 'config': {'dialations': [1, 1, 1, 1], 'conv1_kernel_size': 5, 'bn_momentum': 0.02}, 'in_channels': '${data.in_channels}', 'out_channels': '${data.num_labels}', 'out_fpn': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 20:50:11.309 | WARNING  | utils.utils:load_checkpoint_with_missing_or_exsessive_keys:91 - Key not found, it will be initialized randomly: model.scene_min\n",
      "2025-01-29 20:50:11.310 | WARNING  | utils.utils:load_checkpoint_with_missing_or_exsessive_keys:91 - Key not found, it will be initialized randomly: model.scene_max\n",
      "2025-01-29 20:50:11.439 | WARNING  | utils.utils:load_checkpoint_with_missing_or_exsessive_keys:115 - excessive key: model.scene_min\n",
      "2025-01-29 20:50:11.439 | WARNING  | utils.utils:load_checkpoint_with_missing_or_exsessive_keys:115 - excessive key: model.scene_max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint!\n",
      "Save dir:  experiments/merged_pipline/run_2025-01-29-20-44-08/mask3d\n",
      "Data root:  dataset/data/replica_split\n",
      "Dataset:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 from file office4 ....\n",
      "Shape of mask:  torch.Size([456153, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  94\n",
      "Shape of confidences output:  94\n",
      "Shape of masks_binary output:  94 torch.Size([993008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 from file room2 ....\n",
      "Shape of mask:  torch.Size([318867, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  100\n",
      "Shape of confidences output:  100\n",
      "Shape of masks_binary output:  100 torch.Size([722496])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 from file office0 ....\n",
      "Shape of mask:  torch.Size([265922, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  97\n",
      "Shape of confidences output:  97\n",
      "Shape of masks_binary output:  97 torch.Size([589517])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3 from file office1 ....\n",
      "Shape of mask:  torch.Size([180492, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  95\n",
      "Shape of confidences output:  95\n",
      "Shape of masks_binary output:  95 torch.Size([423007])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4 from file office2 ....\n",
      "Shape of mask:  torch.Size([378125, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  89\n",
      "Shape of confidences output:  89\n",
      "Shape of masks_binary output:  89 torch.Size([858623])\n",
      "Processing batch 5 from file room0 ....\n",
      "Shape of mask:  torch.Size([435468, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  95\n",
      "Shape of confidences output:  95\n",
      "Shape of masks_binary output:  95 torch.Size([954492])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6 from file office3 ....\n",
      "Shape of mask:  torch.Size([515474, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  97\n",
      "Shape of confidences output:  97\n",
      "Shape of masks_binary output:  97 torch.Size([1187140])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Aborted by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7 from file room1 ....\n",
      "Shape of mask:  torch.Size([277142, 100])\n",
      "Shape of logits:  torch.Size([100, 19])\n",
      "Shape of labels output:  94\n",
      "Shape of confidences output:  94\n",
      "Shape of masks_binary output:  94 torch.Size([645512])\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$output_path\"\n",
    "# Run openscene\n",
    "set -x\n",
    "\n",
    "exp_dir=\"$1/openscene\"\n",
    "config=\"./config/openscene/replica/replica_lseg.yaml\"\n",
    "feature_type=distill\n",
    "\n",
    "mkdir -p \"${exp_dir}\"\n",
    "result_dir=\"${exp_dir}\"\n",
    "\n",
    "export PYTHONPATH=\"models/openscene\"\n",
    "python -u models/openscene/run/evaluate_merged.py \\\n",
    "  --config=${config} \\\n",
    "  feature_type ${feature_type} \\\n",
    "  save_folder ${result_dir} \\\n",
    "  2>&1 | tee -a ${exp_dir}/eval-$(date +\"%Y%m%d_%H%M\").log\n",
    "\n",
    "\n",
    "# Run mask3d\n",
    "exp_dir=\"$1/mask3d\"\n",
    "\n",
    "mkdir -p \"${exp_dir}\"\n",
    "result_dir=\"${exp_dir}\"\n",
    "\n",
    "export PYTHONPATH=\".\"\n",
    "python -u models/Mask3D/predict.py \\\n",
    "general.checkpoint='models/Mask3D/checkpoints/scannet/scannet_val.ckpt' \\\n",
    "general.data_dir=\"dataset/data/replica_split\" \\\n",
    "general.save_dir=${result_dir} \\\n",
    "general.split=\"all\" \\\n",
    "general.required_confidence=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dae50e",
   "metadata": {},
   "source": [
    "### Merge point features to get per instance features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/54/blessman/ml3d/experiments/merged_pipline/run_2025-01-29-20-44-08\n",
      "Instance masks:  8\n",
      "Per point features:  8\n",
      "Processing:  office4\n",
      "Masks shape: (94, 993008)\n",
      "Features shape: (993008, 512)\n",
      "Mean instane features: (94, 512)\n",
      "Saved instance features for office4\n",
      "Processing:  room2\n",
      "Masks shape: (100, 722496)\n",
      "Features shape: (722496, 512)\n",
      "Mean instane features: (100, 512)\n",
      "Saved instance features for room2\n",
      "Processing:  office0\n",
      "Masks shape: (97, 589517)\n",
      "Features shape: (589517, 512)\n",
      "Mean instane features: (97, 512)\n",
      "Saved instance features for office0\n",
      "Processing:  office1\n",
      "Masks shape: (95, 423007)\n",
      "Features shape: (423007, 512)\n",
      "Mean instane features: (95, 512)\n",
      "Saved instance features for office1\n",
      "Processing:  office2\n",
      "Masks shape: (89, 858623)\n",
      "Features shape: (858623, 512)\n",
      "Mean instane features: (89, 512)\n",
      "Saved instance features for office2\n",
      "Processing:  room0\n",
      "Masks shape: (95, 954492)\n",
      "Features shape: (954492, 512)\n",
      "Mean instane features: (95, 512)\n",
      "Saved instance features for room0\n",
      "Processing:  office3\n",
      "Masks shape: (97, 1187140)\n",
      "Features shape: (1187140, 512)\n",
      "Mean instane features: (97, 512)\n",
      "Saved instance features for office3\n",
      "Processing:  room1\n",
      "Masks shape: (94, 645512)\n",
      "Features shape: (645512, 512)\n",
      "Mean instane features: (94, 512)\n",
      "Saved instance features for room1\n"
     ]
    }
   ],
   "source": [
    "output_path = experiment.get_current_path()\n",
    "print(output_path)\n",
    "\n",
    "utils.merge_extracted_features(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f82eb3",
   "metadata": {},
   "source": [
    "## Step 2: Reconstruct learned prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb005d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General logic based on Ayaka's code in maple_prompt_scene.py\n",
    "# Will probably need to adjust variable names and paths to match your setup\n",
    "# Should talk about it in our meeting today\n",
    "\n",
    "def reconstruct_learned_prompts(checkpoint_path, class_names, clip_model, cfg):\n",
    "    \"\"\"\n",
    "    Reconstruct learned prompts for inference using the same format as MultiModalPromptLearner.\n",
    "    \"\"\"\n",
    "    # Get config parameters matching training\n",
    "    n_ctx = cfg.TRAINER.MAPLE_PROMPT_SCENE.N_CTX\n",
    "    ctx_init = cfg.TRAINER.MAPLE_PROMPT_SCENE.CTX_INIT\n",
    "    \n",
    "    # Load learned tokens - using same state_dict key as MultiModalPromptLearner\n",
    "    state_dict = torch.load(checkpoint_path)[\"state_dict\"]\n",
    "    ctx = state_dict[\"prompt_learner.ctx\"]  # This matches self.ctx in the learner\n",
    "    \n",
    "    # Process class names\n",
    "    classnames = [name.replace(\"_\", \" \") for name in class_names]\n",
    "    prompts = [f\"{ctx_init} {name}.\" for name in classnames]\n",
    "    \n",
    "    # Tokenize using same function\n",
    "    tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "    \n",
    "    # Get token embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "        # Extract prefix/suffix matching MultiModalPromptLearner attributes\n",
    "        token_prefix = embedding[:, :1, :]  # SOS token\n",
    "        token_suffix = embedding[:, 1 + n_ctx:, :]  # class name + EOS\n",
    "        \n",
    "        # Reconstruct full prompts same way as construct_prompts() method\n",
    "        prompts = torch.cat([\n",
    "            token_prefix,  # (n_cls, 1, dim)\n",
    "            ctx.unsqueeze(0).expand(len(classnames), -1, -1),  # (n_cls, n_ctx, dim)\n",
    "            token_suffix,  # (n_cls, *, dim)\n",
    "        ], dim=1)\n",
    "    \n",
    "    # Encode through text encoder to get CLIP embeddings\n",
    "    text_features = clip_model.encode_text(prompts)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "\n",
    "text_features = reconstruct_learned_prompts(\n",
    "    checkpoint_path=\"path/to/model.pth.tar-2\",\n",
    "    class_names=clip_utils.REPLICA_LABELS,\n",
    "    clip_model=clip_utils.get_clip_model(),\n",
    "    cfg=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1383fa2",
   "metadata": {},
   "source": [
    "## Step 3: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f8e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/test/office4_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/test/room2_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/train/office0_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/train/office1_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/train/office2_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/train/room0_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/val/office3_instance_features.npy',\n",
       " '/cluster/54/nanriayaka/ml3d/experiments/merged_pipline/run_2025-01-28-04-03-04/instance_features/val/room1_instance_features.npy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = experiment.get_current_path()\n",
    "print(output_path)\n",
    "\n",
    "# Load instance feature vectors\n",
    "instance_path = os.path.join(output_path, \"instance_features\")\n",
    "npy_files = utils.get_all_files_in_dir_and_subdir(instance_path, \"npy\")\n",
    "npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0761b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load learned prompt clip embeddings\n",
    "# TODO: load prompts and encode with clip using clip_utils\n",
    "import torch\n",
    "prompt = torch.load(\"models/multimodal-prompt-learning/replica_prompts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7160b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use prompt engineering: a XX in a scene\n",
      "Loading CLIP ViT-B/32 model...\n",
      "Finish loading\n"
     ]
    }
   ],
   "source": [
    "from clip_utils import extract_text_feature, REPLICA_LABELS#, MATTERPORT_LABELS_21\n",
    "labelset = list(REPLICA_LABELS)\n",
    "text_features, new_label_set = extract_text_feature(labelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5e266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a basket.',\n",
       " 'a photo of a bed.',\n",
       " 'a photo of a bench.',\n",
       " 'a photo of a bin.',\n",
       " 'a photo of a blanket.',\n",
       " 'a photo of a blinds.',\n",
       " 'a photo of a book.',\n",
       " 'a photo of a bottle.',\n",
       " 'a photo of a box.',\n",
       " 'a photo of a bowl.',\n",
       " 'a photo of a camera.',\n",
       " 'a photo of a cabinet.',\n",
       " 'a photo of a candle.',\n",
       " 'a photo of a chair.',\n",
       " 'a photo of a clock.',\n",
       " 'a photo of a cloth.',\n",
       " 'a photo of a comforter.',\n",
       " 'a photo of a cushion.',\n",
       " 'a photo of a desk.',\n",
       " 'a photo of a desk-organizer.',\n",
       " 'a photo of a door.',\n",
       " 'a photo of a indoor-plant.',\n",
       " 'a photo of a lamp.',\n",
       " 'a photo of a monitor.',\n",
       " 'a photo of a nightstand.',\n",
       " 'a photo of a panel.',\n",
       " 'a photo of a picture.',\n",
       " 'a photo of a pillar.',\n",
       " 'a photo of a pillow.',\n",
       " 'a photo of a pipe.',\n",
       " 'a photo of a plant-stand.',\n",
       " 'a photo of a plate.',\n",
       " 'a photo of a pot.',\n",
       " 'a photo of a sculpture.',\n",
       " 'a photo of a shelf.',\n",
       " 'a photo of a sofa.',\n",
       " 'a photo of a stool.',\n",
       " 'a photo of a switch.',\n",
       " 'a photo of a table.',\n",
       " 'a photo of a tablet.',\n",
       " 'a photo of a tissue-paper.',\n",
       " 'a photo of a tv-screen.',\n",
       " 'a photo of a tv-stand.',\n",
       " 'a photo of a vase.',\n",
       " 'a photo of a vent.',\n",
       " 'a photo of a wall-plug.',\n",
       " 'a photo of a window.',\n",
       " 'a photo of a rug.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37439799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing office4\n",
      "torch.Size([92, 512])\n",
      "Processing room2\n",
      "torch.Size([100, 512])\n",
      "Processing office0\n",
      "torch.Size([93, 512])\n",
      "Processing office1\n",
      "torch.Size([94, 512])\n",
      "Processing office2\n",
      "torch.Size([95, 512])\n",
      "Processing room0\n",
      "torch.Size([97, 512])\n",
      "Processing office3\n",
      "torch.Size([90, 512])\n",
      "Processing room1\n",
      "torch.Size([94, 512])\n"
     ]
    }
   ],
   "source": [
    "# Classify features\n",
    "import numpy as np\n",
    "from clip_utils import classify_features\n",
    "for file in npy_files:\n",
    "    instance_features = np.load(file)\n",
    "    instance_features = torch.Tensor(instance_features)\n",
    "    \n",
    "    sample_name = os.path.basename(file).split('_')[0]\n",
    "    \n",
    "    print(f\"Processing {sample_name}\")\n",
    "    \n",
    "    # print(prompt['text_embeddings'].shape)\n",
    "    print(instance_features.shape)\n",
    "    \n",
    "    # predicted_classes, confidence_scores = classify_features(text_features, instance_features)\n",
    "    predicted_classes, confidence_scores = classify_features(prompt['text_embeddings'], instance_features)\n",
    "\n",
    "    save_path = os.path.dirname(file)\n",
    "    torch.save(predicted_classes, os.path.join(save_path, f\"{sample_name}_prompt_learning_predicted_classes.pl\"))\n",
    "    torch.save(confidence_scores, os.path.join(save_path, f\"{sample_name}_prompt_learning_confidence_scores.pl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a558b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
