{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: OpenPrompt: Open-Vocabulary 3D Scene Understanding and Instance Segmentation with Adaptive Prompt Learning\n",
    "\n",
    "### Dataset\n",
    "- **Replica Dataset**  \n",
    "  - Download link: [Replica dataset](https://github.com/aminebdj/OpenYOLO3D/blob/main/scripts/get_replica_dataset.sh)\n",
    "\n",
    "### Evaluation Script\n",
    "- **Replica Evaluation Script**  \n",
    "  - Link: [Replica evaluation script](https://github.com/aminebdj/OpenYOLO3D/tree/main/evaluate/replica)\n",
    "\n",
    "### Reference Papers for Prompt Learning\n",
    "1. **Align Your Prompts:** Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization\n",
    "2. **MaPLe:** Multi-modal Prompt Learning\n",
    "\n",
    "### Modifications and Goals\n",
    "1. **Objective:**  \n",
    "   Create an open-vocabulary 3D instance segmentation pipeline.\n",
    "   - Use **OpenScene** for feature extraction.\n",
    "   - Use **Mask3D** for class-agnostic proposal generation.\n",
    "\n",
    "2. **Testing:**  \n",
    "   Evaluate open-vocabulary instance segmentation results on the **Replica dataset**.  \n",
    "   - Metric: **Mean Average Precision (mAP)**  \n",
    "   - Evaluation script: Provided above.\n",
    "\n",
    "3. **Baseline and Improvements:**  \n",
    "   - Start with baseline model results.\n",
    "   - Implement **prompt learning** to improve performance.\n",
    "\n",
    "### Prompt Learning Details\n",
    "- Replace the fixed text features from the **CLIP text encoder** with a **trainable prompt** initialized with a text prompt.\n",
    "- **Training Objective:** Reduce the cosine similarity between visual features of the same object when augmented in different ways (e.g., translations, rotations, or color changes).  \n",
    "- Ensure consistency in visual features of the same object across augmentations by optimizing the learnable prompt during training.\n",
    "\n",
    "### Summary\n",
    "- Develop and test an open-vocabulary 3D instance segmentation pipeline with the specified modifications.\n",
    "- Leverage prompt learning techniques to enhance the baseline model's performance on the **Replica dataset**.\n",
    "- Evaluate using **mAP** as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: follow mask3d installation instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt-get install libopenexr-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorboardx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: install sharedarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install sharedarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the merged pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Openscene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ exp_dir=./experiments/openscene/replica_split\n",
      "+ config=./config/replica/replica.yaml\n",
      "+ feature_type=distill\n",
      "+ mkdir -p ./experiments/openscene/replica_split\n",
      "+ result_dir=./experiments/openscene/replica_split/result_eval\n",
      "+ export PYTHONPATH=models/openscene\n",
      "+ PYTHONPATH=models/openscene\n",
      "+ python -u models/openscene/run/evaluate_merged.py --config=./config/replica/replica.yaml feature_type distill save_folder ./experiments/openscene/replica_split/result_eval\n",
      "++ date +%Y%m%d_%H%M\n",
      "+ tee -a ./experiments/openscene/replica_split/eval-20250118_1506.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__:1.12.1+cu113\n",
      "torch.version.cuda:11.3\n",
      "torch.backends.cudnn.version:8302\n",
      "torch.backends.cudnn.enabled:True\n",
      "[2025-01-18 15:07:37,990 evaluate_merged.py line 159] arch_3d: MinkUNet18A\n",
      "data_root: datasets/data/replica_split\n",
      "data_root_2d_fused_feature: data/replica_multiview_openseg\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:6787\n",
      "distributed: False\n",
      "eval_iou: False\n",
      "feature_2d_extractor: openseg\n",
      "feature_type: distill\n",
      "input_color: False\n",
      "labelset: matterport\n",
      "manual_seed: 3407\n",
      "mark_no_feature_to_unknown: True\n",
      "model_path: https://cvg-data.inf.ethz.ch/openscene/models/matterport_openseg.pth.tar\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "prompt_eng: True\n",
      "rank: 0\n",
      "save_feature_as_numpy: False\n",
      "save_folder: ./experiments/openscene/replica_split/result_eval\n",
      "split: val\n",
      "sync_bn: False\n",
      "test_batch_size: 1\n",
      "test_gpu: [0]\n",
      "test_repeats: 1\n",
      "test_workers: 0\n",
      "use_apex: False\n",
      "use_shm: False\n",
      "vis_input: True\n",
      "vis_pred: True\n",
      "voxel_size: 0.02\n",
      "world_size: 1\n",
      "Use prompt engineering: a XX in a scene\n",
      "Loading CLIP ViT-L/14@336px model...\n",
      "Finish loading\n",
      "[2025-01-18 15:09:14,759 evaluate_merged.py line 279] \n",
      "Evaluation 1 out of 1 runs...\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: datasets/data/replica_split/val/office3_mesh.ply\u001b[0;m\n",
      " 50%|█████     | 1/2 [00:08<00:08,  8.47s/it]\u001b[1;33m[Open3D WARNING] Read PLY failed: A polygon in the mesh could not be decomposed into triangles.\u001b[0;m\n",
      "RPly: Aborted by user\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to read file: datasets/data/replica_split/val/room1_mesh.ply\u001b[0;m\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.61s/it]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run openscene\n",
    "set -x\n",
    "\n",
    "exp_dir=\"./experiments/openscene/replica_split\"\n",
    "config=\"./models/replica/replica.yaml\"\n",
    "feature_type=distill\n",
    "\n",
    "mkdir -p \"${exp_dir}\"\n",
    "result_dir=\"${exp_dir}/result_eval\"\n",
    "\n",
    "export PYTHONPATH=\"models/openscene\"\n",
    "python -u models/openscene/run/evaluate_merged.py \\\n",
    "  --config=${config} \\\n",
    "  feature_type ${feature_type} \\\n",
    "  save_folder ${result_dir} \\\n",
    "  2>&1 | tee -a ${exp_dir}/eval-$(date +\"%Y%m%d_%H%M\").log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLY file loaded successfully!\n",
      "Number of points: 1187140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2e0ce58cee4443b9ac4b20e3a865b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLY file loaded successfully!\n",
      "Number of points: 645512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a35fd0496fe454d9f7deefeef23f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLY file loaded successfully!\n",
      "Number of points: 645512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b8931dac5491fbf92277b8680d310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLY file loaded successfully!\n",
      "Number of points: 1187140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc3a71d48f94b1bae1032c54378bc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import k3d\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def visualize_ply_with_k3d(file_path, point_size=0.05):\n",
    "    \"\"\"\n",
    "    Load a PLY file and visualize it using k3d in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PLY file.\n",
    "        point_size (float): Size of the points in the visualization.\n",
    "    \"\"\"\n",
    "    # Load the PLY file using Open3D\n",
    "    ply_data = o3d.io.read_point_cloud(file_path)\n",
    "    \n",
    "    # Check if the file is loaded correctly\n",
    "    if ply_data.is_empty():\n",
    "        print(\"Failed to load PLY file.\")\n",
    "        return\n",
    "\n",
    "    print(\"PLY file loaded successfully!\")\n",
    "    print(f\"Number of points: {len(ply_data.points)}\")\n",
    "\n",
    "    # Extract points and colors\n",
    "    coords = np.asarray(ply_data.points)  # 3D coordinates\n",
    "    colors = np.asarray(ply_data.colors)  # RGB values (normalized to [0, 1])\n",
    "\n",
    "    # Normalize colors to 0-255 and convert to hexadecimal\n",
    "    colors = (colors * 255).astype(np.uint64)\n",
    "    colors_hex = (colors[:, 0] << 16) + (colors[:, 1] << 8) + colors[:, 2]\n",
    "\n",
    "    # Visualize with k3d\n",
    "    plot = k3d.plot()\n",
    "    point_cloud = k3d.points(positions=coords, point_size=point_size, colors=colors_hex)\n",
    "    plot += point_cloud\n",
    "    return plot\n",
    "\n",
    "# Path to your PLY file\n",
    "file_path = \"experiments/openscene/replica_split/result_eval\"\n",
    "files = glob.glob(os.path.join(file_path, \"*.ply\"))\n",
    "\n",
    "# Call the visualization function\n",
    "for file in files:\n",
    "    plot = visualize_ply_with_k3d(file)\n",
    "    if plot:\n",
    "        plot.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mask3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export OMP_NUM_THREADS=3\n",
    "\n",
    "CURR_DBSCAN=14.0\n",
    "CURR_TOPK=750\n",
    "CURR_QUERY=160\n",
    "CURR_SIZE=54\n",
    "\n",
    "python main_instance_segmentation.py \\\n",
    "general.experiment_name=\"validation_query_${CURR_QUERY}_topk_${CURR_TOPK}_dbscan_${CURR_DBSCAN}_size_${CURR_SIZE}\" \\\n",
    "general.project_name=\"stpls3d_eval\" \\\n",
    "data/datasets=stpls3d \\\n",
    "general.num_targets=15 \\\n",
    "data.num_labels=15 \\\n",
    "data.voxel_size=0.333 \\\n",
    "data.num_workers=10 \\\n",
    "data.cache_data=true \\\n",
    "data.cropping_v1=false \\\n",
    "general.reps_per_epoch=100 \\\n",
    "model.num_queries=${CURR_QUERY} \\\n",
    "general.on_crops=true \\\n",
    "model.config.backbone._target_=models.Res16UNet18B \\\n",
    "general.train_mode=false \\\n",
    "general.checkpoint=\"checkpoints/stpls3d/stpls3d_val.ckpt\" \\\n",
    "data.crop_length=${CURR_SIZE} \\\n",
    "general.eval_inner_core=50.0 \\\n",
    "general.topk_per_image=${CURR_TOPK} \\\n",
    "general.use_dbscan=true \\\n",
    "general.dbscan_eps=${CURR_DBSCAN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export OMP_NUM_THREADS=3\n",
    "export WANDB_MODE=offline\n",
    "export WANDB_MODE=disabled\n",
    "\n",
    "CURR_DBSCAN=14.0\n",
    "CURR_TOPK=750\n",
    "CURR_QUERY=160\n",
    "CURR_SIZE=54\n",
    "\n",
    "python main_instance_segmentation.py \\\n",
    "data/datasets=stpls3d \\\n",
    "general.train_mode=false \\\n",
    "general.checkpoint=\"checkpoints/stpls3d/stpls3d_val.ckpt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
