{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an anaconda environment called openscene as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install libopenexr-dev # for linux\n",
    "conda create -n openscene python=3.8\n",
    "conda activate openscene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: install PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: install MinkowskiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install gcc python3 openblas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: install all the remaining dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 (optional): if you need to run multi-view feature fusion with OpenSeg (especially for your own dataset), remember to install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install clip\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd demo && \\\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/demo_data.zip && \\\n",
    "unzip demo_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd gaps\n",
    "make VERBOSE=1 > make_log.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Compile gaps library\n",
    "cd gaps\n",
    "make\n",
    "\n",
    "# Download and compile RNNets into gaps/pkgs/RNNets\n",
    "cd pkgs\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/RNNets.zip\n",
    "unzip RNNets.zip\n",
    "cd RNNets\n",
    "make\n",
    "\n",
    "# Download and compile osview into gaps/apps/osview\n",
    "# The executable will be in gaps/bin/x86_64/osview\n",
    "cd ../apps\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/osview.zip\n",
    "unzip osview.zip\n",
    "cd osview\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure you are under demo/, and you can simply run to have fun with the interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../demo\n",
    "./run_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads pre-processed datasets used in the OpenScene project.\n",
    "Choose from the following options:\n",
    "- 0 - ScanNet 3D (point clouds with GT semantic labels)\n",
    "- 1 - ScanNet 2D (RGB-D images with camera poses)\n",
    "- 2 - Matterport 3D (point clouds with GT semantic labels)\n",
    "- 3 - Matterport 2D (RGB-D images with camera poses)\n",
    "- 4 - nuScenes 3D - Validation Set (lidar point clouds with GT semantic labels)\n",
    "- 5 - nuScenes 3D - Training Set (lidar point clouds with GT semantic labels), 379.9G\n",
    "- 6 - nuScenes 2D (RGB images with camera poses)\n",
    "- 7 - Replica 3D (point clouds)\n",
    "- 8 - Replica 2D (RGB-D images)\n",
    "- 9 - Matterport 3D with top 40 NYU classes\n",
    "- 10 - Matterport 3D with top 80 NYU classes\n",
    "- 11- Matterport 3D with top 160 NYU classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script downloads pre-processed datasets used in the OpenScene project.\n",
      "Choose from the following options:\n",
      "0 - ScanNet 3D (point clouds with GT semantic labels)\n",
      "1 - ScanNet 2D (RGB-D images with camera poses)\n",
      "2 - Matterport 3D (point clouds with GT semantic labels)\n",
      "3 - Matterport 2D (RGB-D images with camera poses)\n",
      "4 - nuScenes 3D - Validation Set (lidar point clouds with GT semantic labels)\n",
      "5 - nuScenes 3D - Training Set (lidar point clouds with GT semantic labels), 379.9G\n",
      "6 - nuScenes 2D (RGB images with camera poses)\n",
      "7 - Replica 3D (point clouds)\n",
      "8 - Replica 2D (RGB-D images)\n",
      "9 - Matterport 3D with top 40 NYU classes\n",
      "10 - Matterport 3D with top 80 NYU classes\n",
      "11- Matterport 3D with top 160 NYU classes\n",
      "You chose 7: Replica 3D\n",
      "Start downloading ...\n",
      "--2025-01-09 10:16:06--  https://cvg-data.inf.ethz.ch/openscene/data/replica_processed/replica_3d.zip\n",
      "Resolving cvg-data.inf.ethz.ch (cvg-data.inf.ethz.ch)... 129.132.114.72\n",
      "Connecting to cvg-data.inf.ethz.ch (cvg-data.inf.ethz.ch)|129.132.114.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102879015 (98M) [application/zip]\n",
      "Saving to: ‘replica_3d.zip’\n",
      "\n",
      "replica_3d.zip      100%[===================>]  98.11M  18.8MB/s    in 8.6s    \n",
      "\n",
      "2025-01-09 10:16:15 (11.4 MB/s) - ‘replica_3d.zip’ saved [102879015/102879015]\n",
      "\n",
      "Done! Start unzipping ...\n",
      "Archive:  replica_3d.zip\n",
      "   creating: replica_3d/\n",
      "  inflating: replica_3d/office0.pth  \n",
      "  inflating: replica_3d/room2.pth    \n",
      "  inflating: replica_3d/office2.pth  \n",
      "  inflating: replica_3d/room0.pth    \n",
      "  inflating: replica_3d/office4.pth  \n",
      "  inflating: replica_3d/office3.pth  \n",
      "  inflating: replica_3d/office1.pth  \n",
      "  inflating: replica_3d/room1.pth    \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Change the number after !echo to download the respective dataset\n",
    "!echo \"7\" | bash scripts/download_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads multi-view fused features used in the OpenScene project.\n",
    "Choose from the following options:\n",
    "- 0 - ScanNet - Multi-view fused OpenSeg features, train/val (234.8G)\n",
    "- 1 - ScanNet - Multi-view fused LSeg features, train/val (175.8G)\n",
    "- 2 - Matterport - Multi-view fused OpenSeg features, train/val (198.3G)\n",
    "- 3 - Matterport - Multi-view fused OpenSeg features, test set (66.7G)\n",
    "- 4 - Replica - Multi-view fused OpenSeg features (9.0G)\n",
    "- 5 - Matterport - Multi-view fused LSeg features (coming)\n",
    "- 6 - nuScenes - Multi-view fused OpenSeg features, validation set (165G) \n",
    "- 7 - nuScenes - Multi-view fused LSeg features (coming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number after !echo to download the respective dataset\n",
    "!echo \"4\" | bash scripts/download_fused_features.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have installed the environment and obtained the processed 3D data and multi-view fused features, you are ready to run our OpenScene disilled/ensemble model for 3D semantic segmentation, or distill your own model from scratch.\n",
    "\n",
    "### Evaluation for 3D Semantic Segmentation with a Pre-defined Labelsets\n",
    "\n",
    "Here you can evaluate OpenScene features on different dataset (ScanNet/Matterport3D/nuScenes/Replica) that have pre-defined labelsets. We already include the following labelsets in label_constants.py:\n",
    "\n",
    "- ScanNet 20 classes (wall, door, chair, ...)\n",
    "- Matterport3D 21 classes (ScanNet 20 classes + floor)\n",
    "- Matterport top 40, 80, 160 NYU classes (more rare object classes)\n",
    "- nuScenes 16 classes (road, bicycle, sidewalk, ...)\n",
    "  \n",
    "The general command to run evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ exp_dir=experiments/replica_openseg\n",
      "+ config=config/replica/ours_openseg_pretrained.yaml\n",
      "+ feature_type=ensemble\n",
      "+ mkdir -p experiments/replica_openseg\n",
      "+ result_dir=experiments/replica_openseg/result_eval\n",
      "+ export PYTHONPATH=.\n",
      "+ python -u run/evaluate.py --config=config/replica/ours_openseg_pretrained.yaml feature_type ensemble save_folder experiments/replica_openseg/result_eval\n",
      "+ date +%Y%m%d_%H%M\n",
      "+ tee -a experiments/replica_openseg/eval-20250109_1029.log\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/University/Master/1.Semester/Machine Learning for 3D Geometry/project/ml3d/models/openscene/run/evaluate.py\", line 23, in <module>\n",
      "    from run.distill import get_model\n",
      "  File \"/home/simon/University/Master/1.Semester/Machine Learning for 3D Geometry/project/ml3d/models/openscene/run/distill.py\", line 24, in <module>\n",
      "    from dataset.feature_loader import FusedFeatureLoader, collation_fn\n",
      "  File \"/home/simon/University/Master/1.Semester/Machine Learning for 3D Geometry/project/ml3d/models/openscene/dataset/feature_loader.py\", line 8, in <module>\n",
      "    import SharedArray as SA\n",
      "ModuleNotFoundError: No module named 'SharedArray'\n"
     ]
    }
   ],
   "source": [
    "!sh run/eval.sh experiments/replica_openseg config/replica/ours_openseg_pretrained.yaml ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mask3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
