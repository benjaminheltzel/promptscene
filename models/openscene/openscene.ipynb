{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an anaconda environment called openscene as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install libopenexr-dev # for linux\n",
    "conda create -n openscene python=3.8\n",
    "conda activate openscene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: install PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: install MinkowskiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install gcc python3 openblas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: install all the remaining dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 (optional): if you need to run multi-view feature fusion with OpenSeg (especially for your own dataset), remember to install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install clip\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd demo && \\\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/demo_data.zip && \\\n",
    "unzip demo_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd gaps\n",
    "make VERBOSE=1 > make_log.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Compile gaps library\n",
    "cd gaps\n",
    "make\n",
    "\n",
    "# Download and compile RNNets into gaps/pkgs/RNNets\n",
    "cd pkgs\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/RNNets.zip\n",
    "unzip RNNets.zip\n",
    "cd RNNets\n",
    "make\n",
    "\n",
    "# Download and compile osview into gaps/apps/osview\n",
    "# The executable will be in gaps/bin/x86_64/osview\n",
    "cd ../apps\n",
    "wget https://cvg-data.inf.ethz.ch/openscene/demo/osview.zip\n",
    "unzip osview.zip\n",
    "cd osview\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure you are under demo/, and you can simply run to have fun with the interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../demo\n",
    "./run_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads pre-processed datasets used in the OpenScene project.\n",
    "Choose from the following options:\n",
    "- 0 - ScanNet 3D (point clouds with GT semantic labels)\n",
    "- 1 - ScanNet 2D (RGB-D images with camera poses)\n",
    "- 2 - Matterport 3D (point clouds with GT semantic labels)\n",
    "- 3 - Matterport 2D (RGB-D images with camera poses)\n",
    "- 4 - nuScenes 3D - Validation Set (lidar point clouds with GT semantic labels)\n",
    "- 5 - nuScenes 3D - Training Set (lidar point clouds with GT semantic labels), 379.9G\n",
    "- 6 - nuScenes 2D (RGB images with camera poses)\n",
    "- 7 - Replica 3D (point clouds)\n",
    "- 8 - Replica 2D (RGB-D images)\n",
    "- 9 - Matterport 3D with top 40 NYU classes\n",
    "- 10 - Matterport 3D with top 80 NYU classes\n",
    "- 11- Matterport 3D with top 160 NYU classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script downloads pre-processed datasets used in the OpenScene project.\n",
      "Choose from the following options:\n",
      "0 - ScanNet 3D (point clouds with GT semantic labels)\n",
      "1 - ScanNet 2D (RGB-D images with camera poses)\n",
      "2 - Matterport 3D (point clouds with GT semantic labels)\n",
      "3 - Matterport 2D (RGB-D images with camera poses)\n",
      "4 - nuScenes 3D - Validation Set (lidar point clouds with GT semantic labels)\n",
      "5 - nuScenes 3D - Training Set (lidar point clouds with GT semantic labels), 379.9G\n",
      "6 - nuScenes 2D (RGB images with camera poses)\n",
      "7 - Replica 3D (point clouds)\n",
      "8 - Replica 2D (RGB-D images)\n",
      "9 - Matterport 3D with top 40 NYU classes\n",
      "10 - Matterport 3D with top 80 NYU classes\n",
      "11- Matterport 3D with top 160 NYU classes\n",
      "You chose 7: Replica 3D\n",
      "Start downloading ...\n",
      "--2025-01-09 10:16:06--  https://cvg-data.inf.ethz.ch/openscene/data/replica_processed/replica_3d.zip\n",
      "Resolving cvg-data.inf.ethz.ch (cvg-data.inf.ethz.ch)... 129.132.114.72\n",
      "Connecting to cvg-data.inf.ethz.ch (cvg-data.inf.ethz.ch)|129.132.114.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102879015 (98M) [application/zip]\n",
      "Saving to: ‘replica_3d.zip’\n",
      "\n",
      "replica_3d.zip      100%[===================>]  98.11M  18.8MB/s    in 8.6s    \n",
      "\n",
      "2025-01-09 10:16:15 (11.4 MB/s) - ‘replica_3d.zip’ saved [102879015/102879015]\n",
      "\n",
      "Done! Start unzipping ...\n",
      "Archive:  replica_3d.zip\n",
      "   creating: replica_3d/\n",
      "  inflating: replica_3d/office0.pth  \n",
      "  inflating: replica_3d/room2.pth    \n",
      "  inflating: replica_3d/office2.pth  \n",
      "  inflating: replica_3d/room0.pth    \n",
      "  inflating: replica_3d/office4.pth  \n",
      "  inflating: replica_3d/office3.pth  \n",
      "  inflating: replica_3d/office1.pth  \n",
      "  inflating: replica_3d/room1.pth    \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Change the number after !echo to download the respective dataset\n",
    "!echo \"7\" | bash scripts/download_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads multi-view fused features used in the OpenScene project.\n",
    "Choose from the following options:\n",
    "- 0 - ScanNet - Multi-view fused OpenSeg features, train/val (234.8G)\n",
    "- 1 - ScanNet - Multi-view fused LSeg features, train/val (175.8G)\n",
    "- 2 - Matterport - Multi-view fused OpenSeg features, train/val (198.3G)\n",
    "- 3 - Matterport - Multi-view fused OpenSeg features, test set (66.7G)\n",
    "- 4 - Replica - Multi-view fused OpenSeg features (9.0G)\n",
    "- 5 - Matterport - Multi-view fused LSeg features (coming)\n",
    "- 6 - nuScenes - Multi-view fused OpenSeg features, validation set (165G) \n",
    "- 7 - nuScenes - Multi-view fused LSeg features (coming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number after !echo to download the respective dataset\n",
    "!echo \"4\" | bash scripts/download_fused_features.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have installed the environment and obtained the processed 3D data and multi-view fused features, you are ready to run our OpenScene disilled/ensemble model for 3D semantic segmentation, or distill your own model from scratch.\n",
    "\n",
    "### Evaluation for 3D Semantic Segmentation with a Pre-defined Labelsets\n",
    "\n",
    "Here you can evaluate OpenScene features on different dataset (ScanNet/Matterport3D/nuScenes/Replica) that have pre-defined labelsets. We already include the following labelsets in label_constants.py:\n",
    "\n",
    "- ScanNet 20 classes (wall, door, chair, ...)\n",
    "- Matterport3D 21 classes (ScanNet 20 classes + floor)\n",
    "- Matterport top 40, 80, 160 NYU classes (more rare object classes)\n",
    "- nuScenes 16 classes (road, bicycle, sidewalk, ...)\n",
    "  \n",
    "The general command to run evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ exp_dir=experiments/replica_openseg\n",
      "+ config=config/replica/ours_openseg_pretrained.yaml\n",
      "+ feature_type=ensemble\n",
      "+ mkdir -p experiments/replica_openseg\n",
      "+ result_dir=experiments/replica_openseg/result_eval\n",
      "+ export PYTHONPATH=.\n",
      "+ python -u run/evaluate.py --config=config/replica/ours_openseg_pretrained.yaml feature_type ensemble save_folder experiments/replica_openseg/result_eval\n",
      "+ date +%Y%m%d_%H%M\n",
      "+ tee -a experiments/replica_openseg/eval-20250114_1512.log\n",
      "torch.__version__:1.12.1+cu113\n",
      "torch.version.cuda:11.3\n",
      "torch.backends.cudnn.version:8302\n",
      "torch.backends.cudnn.enabled:True\n",
      "[2025-01-14 15:12:33,947 evaluate.py line 154] arch_3d: MinkUNet18A\n",
      "data_root: data/replica_3d\n",
      "data_root_2d_fused_feature: data/replica_multiview_openseg\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:6787\n",
      "distributed: False\n",
      "eval_iou: False\n",
      "feature_2d_extractor: openseg\n",
      "feature_type: ensemble\n",
      "input_color: False\n",
      "labelset: matterport\n",
      "manual_seed: 3407\n",
      "mark_no_feature_to_unknown: True\n",
      "model_path: https://cvg-data.inf.ethz.ch/openscene/models/matterport_openseg.pth.tar\n",
      "multiprocessing_distributed: False\n",
      "ngpus_per_node: 1\n",
      "prompt_eng: True\n",
      "rank: 0\n",
      "save_feature_as_numpy: False\n",
      "save_folder: experiments/replica_openseg/result_eval\n",
      "split: None\n",
      "sync_bn: False\n",
      "test_batch_size: 1\n",
      "test_gpu: [0]\n",
      "test_repeats: 1\n",
      "test_workers: 0\n",
      "use_apex: False\n",
      "use_shm: False\n",
      "vis_input: True\n",
      "vis_pred: True\n",
      "voxel_size: 0.02\n",
      "world_size: 1\n",
      "Use prompt engineering: a XX in a scene\n",
      "Loading CLIP ViT-L/14@336px model...\n",
      "Finish loading\n",
      "[2025-01-14 15:12:42,443 evaluate.py line 267] \n",
      "Evaluation 1 out of 1 runs...\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]test\n",
      "data/replica_multiview_openseg/office0_0.pt\n",
      " 12%|█▎        | 1/8 [00:03<00:23,  3.34s/it]test\n",
      "data/replica_multiview_openseg/office1_0.pt\n",
      " 25%|██▌       | 2/8 [00:05<00:15,  2.54s/it]test\n",
      "data/replica_multiview_openseg/office2_0.pt\n",
      " 38%|███▊      | 3/8 [00:09<00:15,  3.13s/it]test\n",
      "data/replica_multiview_openseg/office3_0.pt\n",
      " 50%|█████     | 4/8 [00:14<00:15,  4.00s/it]test\n",
      "data/replica_multiview_openseg/office4_0.pt\n",
      " 62%|██████▎   | 5/8 [00:18<00:12,  4.10s/it]test\n",
      "data/replica_multiview_openseg/room0_0.pt\n",
      " 75%|███████▌  | 6/8 [00:22<00:08,  4.12s/it]test\n",
      "data/replica_multiview_openseg/room1_0.pt\n",
      " 88%|████████▊ | 7/8 [00:25<00:03,  3.70s/it]test\n",
      "data/replica_multiview_openseg/room2_0.pt\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "!sh run/eval.sh experiments/replica_openseg config/replica/ours_openseg_pretrained.yaml ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
